# -*- coding: utf-8 -*-
"""My_Movie_Recommender.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JRUZFZmSowfvU_pgU0FgrRYd647-6Hug
"""

import os
from google.colab import files
import tensorflow.compat.v1 as tf
tf.compat.v1.disable_eager_execution()
tf.compat.v1.get_default_graph() 

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.python.tools import freeze_graph
from tensorflow.python.tools import optimize_for_inference_lib

# Load the movies dataset and also pass header=None since files don't contain any headers
from google.colab import files # Use to load data on Google Colab
uploaded = files.upload() # Use to load data on Google Colab

# Load the movies dataset and also pass header=None since files don't contain any headers
movies_df = pd.read_csv('movies.dat', sep='::', header=None, engine='python')
#print(movies_df.head())

from google.colab import files # Use to load data on Google Colab
uploaded = files.upload() # Use to load data on Google Colab

# Load the ratings dataset
ratings_df = pd.read_csv('ratings.dat', sep='::', header=None, engine='python')
#print(ratings_df.head())

# Lets rename our columns in these data frames so we can convey their data better
movies_df.columns = ['MovieID', 'Title', 'Genres']
ratings_df.columns = ['UserID', 'MovieID', 'Rating', 'Timestamp']

# Verify the changes done to the dataframes
#print(movies_df.head())
#print(ratings_df.head())

# Data Correction and Formatting
#print('The Number of Movies in Dataset', len(movies_df))
#Our Movie ID's vary from 1 to 3952 while we have 3883 movies. 
# Due to this, we won't be able to index movies through their ID since we would get memory indexing errors. 
#To amend we can create a column that shows the spot in our list that particular movie is in:


movies_df['List Index'] = movies_df.index
#print(movies_df.head())

#Merge movies_df with ratings_df by MovieID
merged_df = movies_df.merge(ratings_df, on='MovieID')
# Drop unnecessary columns
merged_df = merged_df.drop('Timestamp', axis=1)

# Display the result
#print(merged_df.head())

# Lets Group up the Users by their user ID's
user_Group = merged_df.groupby('UserID')
#print(user_Group.head())

#Formatting the data into input for the RBM. 
#Store the normalized users ratings into a list of lists called trX.
 #Amount of users used for training
amountOfUsedUsers = 1000
# Creating the training list
trX = []

# For each user in the group
for userID, curUser in user_Group:

    # Create a temp that stores every movie's rating
    temp = [0]*len(movies_df)

    # For each movie in curUser's movie list
    for num, movie in curUser.iterrows():

        # Divide the rating by 5 and store it
        temp[movie['List Index']] = movie['Rating']/5.0

    # Add the list of ratings into the training list
    trX.append(temp)
   # Check to see if we finished adding in the amount of users for training
    if amountOfUsedUsers == 0:
        break
    amountOfUsedUsers -= 1

# Setting the models Parameters

hiddenUnits = 50
visibleUnits = len(movies_df)
#vb = tf.placeholder(tf.float32, [visibleUnits])  # Number of unique movies
#hb = tf.placeholder(tf.float32, [hiddenUnits])  # Number of features were going to learn
#W = tf.placeholder(tf.float32, [visibleUnits, hiddenUnits])  # Weight Matrix  
W = tf.Variable(tf.random_normal(shape=(visibleUnits, hiddenUnits)), name='weight')
vb = tf.Variable(tf.random_normal(shape=(visibleUnits ,)), name='visiblebias')
hb = tf.Variable(tf.random_normal(shape=(hiddenUnits ,  )), name='hiddenbias')
# Phase 1: Input Processing
v0 = tf.placeholder("float", [None, visibleUnits] , name='input')
#v0 = tf.Variable(tf.random_normal(shape=(None ,visibleUnits)), name='v0')
_h0 = tf.nn.sigmoid(tf.matmul(v0, W) + hb ) # Visible layer activation
h0 = tf.nn.relu(tf.sign(_h0 - tf.random_uniform(tf.shape(_h0))))  # Gibb's Sampling
# Phase 2: Reconstruction
_v1 = tf.nn.sigmoid(tf.matmul(h0, tf.transpose(W)) + vb )  # Hidden layer activation
v1 = tf.nn.relu(tf.sign(_v1 - tf.random_uniform(tf.shape(_v1))))
h1 = tf.nn.sigmoid(tf.matmul(v1, W) + hb )
#Set RBM Training Parameters """

# Learning rate
alpha = 1.0
# Create the gradients
w_pos_grad = tf.matmul(tf.transpose(v0), h0)
w_neg_grad = tf.matmul(tf.transpose(v1), h1)

# Calculate the Contrastive Divergence to maximize
CD = (w_pos_grad - w_neg_grad) / tf.to_float(tf.shape(v0)[0])

# Create methods to update the weights and biases
update_w = W + alpha * CD
update_vb = vb + alpha * tf.reduce_mean(v0 - v1, 0)
update_hb = hb + alpha * tf.reduce_mean(h0 - h1, 0)
# Set the error function, here we use Mean Absolute Error Function
err = v0 - v1
err_sum = tf.reduce_mean(err*err)
#""" Initialize our Variables with Zeroes using Numpy Library """

# Current weight
cur_w = np.zeros([visibleUnits, hiddenUnits])
# Current visible unit biases
cur_vb = np.zeros([visibleUnits], np.float32 )
# Current hidden unit biases
cur_hb = np.zeros([hiddenUnits], np.float32)

# Previous weight
prv_w = np.zeros([visibleUnits, hiddenUnits], np.float32)

# Previous visible unit biases
prv_vb = np.zeros([visibleUnits], np.float32)

# Previous hidden unit biases
prv_hb = np.zeros([hiddenUnits], np.float32)

# the tensorflow saver & session
 
saver=tf.train.Saver()
save_path=(r"/content/")
model_save="model.ckpt"
sess = tf.Session()
sess.run(tf.global_variables_initializer())
# Train RBM with 3 Epochs, with Each Epoch using 10 batches with size 100, After training print out the error by epoch
epochs = 3
batchsize =100
errors = []

for i in range(epochs):
    for start, end in zip(range(0, len(trX), batchsize), range(batchsize, len(trX), batchsize)):
        

        batch = trX[start:end]
      
        cur_w = sess.run(update_w, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})
        cur_vb = sess.run(update_vb, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})
        cur_hb = sess.run(update_hb, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})
        prv_w = cur_w
        prv_vb = cur_vb
        prv_hb = cur_hb
    errors.append(sess.run(err_sum, feed_dict={v0: trX, W: cur_w, vb: cur_vb, hb: cur_hb}))
    print(errors[-1])
plt.plot(errors)
plt.ylabel('Error')
plt.xlabel('Epoch')
plt.show()

#Recommendation system
# Select the input User

inputUser = trX

# Feeding in the User and Reconstructing the input
hh0 = tf.nn.sigmoid(tf.matmul(v0, W) + hb,name='output')
vv1 = tf.nn.sigmoid(tf.matmul(hh0, tf.transpose(W)) + vb)
feed = sess.run(hh0, feed_dict={v0: inputUser, W: prv_w, hb: prv_hb})
rec = sess.run(vv1, feed_dict={hh0: feed, W: prv_w, vb: prv_vb})

# List the 20 most recommended movies for our mock user by sorting it by their scores given by our model.
scored_movies_df_50 = movies_df
scored_movies_df_50["Recommendation Score"] = rec[0]
print(scored_movies_df_50.sort_values(["Recommendation Score"], ascending=False).head(10))
saver.save(sess,model_save) #saving the model
tf.train.write_graph(sess.graph_def, save_path, 'mymodel.pbtxt') #saving the model's tensorflow graph definition

#Freeeze the graphe 

def freeze_graph(model_dir, output_node_names):
  """Extract the sub graph defined by the output nodes and convert 
  all its variables into constant 
  Args:
      model_dir: the root folder containing the checkpoint state file
      output_node_names: a string, containing all the output node's names, 
                          comma separated
                        """
  if not tf.gfile.Exists(model_dir):
    raise AssertionError(
      "Export directory doesn't exists. Please specify an export "
      "directory: %s" % model_dir)

  if not output_node_names:
    print("You need to supply the name of a node to --output_node_names.")
    return -1

  # We retrieve our checkpoint fullpath
  checkpoint = tf.train.get_checkpoint_state(model_dir)
  input_checkpoint = checkpoint.model_checkpoint_path
    
  # We precise the file fullname of our freezed graph
  absolute_model_dir = "/".join(input_checkpoint.split('/')[:-1])
  output_graph = absolute_model_dir + "/frozen_model.pb"

  # We clear devices to allow TensorFlow to control on which device it will load operations
  clear_devices = True

  # We start a session using a temporary fresh Graph
  with tf.Session(graph=tf.Graph()) as sess:
    # We import the meta graph in the current default Graph
    saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)

    # We restore the weights
    saver.restore(sess, input_checkpoint)

    # We use a built-in TF helper to export variables to constants
    output_graph_def = tf.graph_util.convert_variables_to_constants(
      sess, # The session is used to retrieve the weights
      tf.get_default_graph().as_graph_def(), # The graph_def is used to retrieve the nodes 
      output_node_names.split(",") # The output node names are used to select the usefull nodes
    ) 

    # Finally we serialize and dump the output graph to the filesystem
    with tf.gfile.GFile(output_graph, "wb") as f:
      f.write(output_graph_def.SerializeToString())
    print("%d ops in the final graph." % len(output_graph_def.node))

  return output_graph_def

model_dir=(r'/content/')
output_node_names='output'
freeze_graph(model_dir, output_node_names )

inputGraph = tf.GraphDef()
with tf.gfile.Open('frozen_model.pb', "rb") as f:
  data2read = f.read()
  inputGraph.ParseFromString(data2read)
  
outputGraph = optimize_for_inference_lib.optimize_for_inference(
              inputGraph,
              ["input"], # an array of the input node(s)
              ["output"], # an array of output nodes
              tf.int32.as_datatype_enum)

# Save the optimized graph'test.pb'
f = tf.gfile.FastGFile('OptimizedGraph.pb', "w")
f.write(outputGraph.SerializeToString())

graph_def_file = (r'/content/frozen_model.pb')  ##Your frozen graph

input_arrays = ['input']         ##Input Node
output_arrays = ['output'] ##Output Node

converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, 
                                                      input_arrays, 
                                                      output_arrays)

tflite_model = converter.convert()
open("converted_model.tflite","wb").write(tflite_model)

import tensorflow as tf
save_path = "/content/"
interpreter = tf.lite.Interpreter(model_path=save_path+"converted_model.tflite")